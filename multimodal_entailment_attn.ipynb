{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multimodal_entailment_attn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP1+j0afxAA3frVwha34wT2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayakpaul/Multimodal-Entailment-Baseline/blob/main/multimodal_entailment_attn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVBuqCa2P5hM"
      },
      "source": [
        "## Setup and data collection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L2Uas2whxM4"
      },
      "source": [
        "!pip install -q tensorflow_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqjVLu_uh4wJ"
      },
      "source": [
        "!wget -q https://github.com/sayakpaul/Multimodal-Entailment-Baseline/releases/download/v1.0.0/tweet_images.tar.gz\n",
        "!tar xf tweet_images.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58cNoAt5P8Yj"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYqTcaAnh5_N"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from tensorflow import keras\n",
        "\n",
        "tf.random.set_seed(13)\n",
        "np.random.seed(13)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT2XrM3YQBpu"
      },
      "source": [
        "## Data reading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiVz7oyQh9T1",
        "outputId": "d5595bbf-c2e0-422d-9794-93980e4ff002"
      },
      "source": [
        "train_df = pd.read_csv(\"https://github.com/sayakpaul/Multimodal-Entailment-Baseline/raw/main/csvs/train_df.csv\")\n",
        "val_df = pd.read_csv(\"https://github.com/sayakpaul/Multimodal-Entailment-Baseline/raw/main/csvs/val_df.csv\")\n",
        "test_df = pd.read_csv(\"https://github.com/sayakpaul/Multimodal-Entailment-Baseline/raw/main/csvs/test_df.csv\")\n",
        "\n",
        "print(f\"Total training examples: {len(train_df)}\")\n",
        "print(f\"Total validation examples: {len(val_df)}\")\n",
        "print(f\"Total test examples: {len(test_df)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total training examples: 1197\n",
            "Total validation examples: 63\n",
            "Total test examples: 140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbrSSXttQDUQ"
      },
      "source": [
        "## Data input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uGoijH4iIL2"
      },
      "source": [
        "# Define TF Hub paths to the BERT encoder and its preprocessor.\n",
        "bert_model_path = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1\"\n",
        "bert_preprocess_path = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4-uBN1yiO-W"
      },
      "source": [
        "# Reference:\n",
        "# https://www.tensorflow.org/text/tutorials/bert_glue\n",
        "\n",
        "def make_bert_preprocess_model(sentence_features, seq_length=128):\n",
        "  \"\"\"Returns Model mapping string features to BERT inputs.\n",
        "\n",
        "  Args:\n",
        "    sentence_features: a list with the names of string-valued features.\n",
        "    seq_length: an integer that defines the sequence length of BERT inputs.\n",
        "\n",
        "  Returns:\n",
        "    A Keras Model that can be called on a list or dict of string Tensors\n",
        "    (with the order or names, resp., given by sentence_features) and\n",
        "    returns a dict of tensors for input to BERT.\n",
        "  \"\"\"\n",
        "\n",
        "  input_segments = [\n",
        "      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
        "      for ft in sentence_features]\n",
        "\n",
        "  # Tokenize the text to word pieces.\n",
        "  bert_preprocess = hub.load(bert_preprocess_path)\n",
        "  tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n",
        "  segments = [tokenizer(s) for s in input_segments]\n",
        "\n",
        "  # Optional: Trim segments in a smart way to fit seq_length.\n",
        "  # Simple cases (like this example) can skip this step and let\n",
        "  # the next step apply a default truncation to approximately equal lengths.\n",
        "  truncated_segments = segments\n",
        "\n",
        "  # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
        "  # are model-dependent, so this gets loaded from the SavedModel.\n",
        "  packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
        "                          arguments=dict(seq_length=seq_length),\n",
        "                          name='packer')\n",
        "  model_inputs = packer(truncated_segments)\n",
        "  return keras.Model(input_segments, model_inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRXSdrB6iQlN"
      },
      "source": [
        "bert_preprocess_model = make_bert_preprocess_model(['text_1', 'text_2'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXM6fUohiS7s"
      },
      "source": [
        "# Reference:\n",
        "# https://keras.io/examples/structured_data/structured_data_classification_from_scratch/\n",
        "def dataframe_to_dataset(dataframe):\n",
        "    columns = [\"image_1_path\", \"image_2_path\", \"text_1\", \"text_2\", \"label_idx\"]\n",
        "    dataframe = dataframe[columns].copy()\n",
        "    labels = dataframe.pop(\"label_idx\")\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "    return ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXTZDw1iiVBu"
      },
      "source": [
        "resize = (128, 128)\n",
        "bert_input_features = ['input_word_ids', 'input_type_ids', 'input_mask']\n",
        "\n",
        "def read_resize(image_path):\n",
        "    extension = tf.strings.split(image_path)[-1]\n",
        "\n",
        "    image = tf.io.read_file(image_path)\n",
        "    if extension == b\"jpg\":\n",
        "        image = tf.image.decode_jpeg(image, 3)\n",
        "    else:\n",
        "        image = tf.image.decode_png(image, 3)\n",
        "    image = tf.image.resize(image, resize)\n",
        "    return image\n",
        "\n",
        "def preprocess_text(text_1, text_2):\n",
        "    text_1 = tf.convert_to_tensor([text_1])\n",
        "    text_2 = tf.convert_to_tensor([text_2])\n",
        "    output = bert_preprocess_model([text_1, text_2])\n",
        "    output = {feature: tf.squeeze(output[feature]) \n",
        "        for feature in bert_input_features}\n",
        "    return output\n",
        "\n",
        "def preprocess(sample):\n",
        "    image_1 = read_resize(sample[\"image_1_path\"])\n",
        "    image_2 = read_resize(sample[\"image_2_path\"])\n",
        "    text = preprocess_text(sample[\"text_1\"], sample[\"text_2\"])\n",
        "    return {\"image_1\": image_1, \"image_2\": image_2, \"text\": text}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlIZB3tTiWxx"
      },
      "source": [
        "batch_size = 32\n",
        "auto = tf.data.AUTOTUNE\n",
        "\n",
        "def prepare_dataset(df, training=True):\n",
        "    ds = dataframe_to_dataset(df)\n",
        "    if training:\n",
        "        ds = ds.shuffle(len(train_df))\n",
        "    ds = ds.map(lambda x, y: (preprocess(x), y))\n",
        "    ds = ds.batch(batch_size).prefetch(auto)\n",
        "    return ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diJgdkE0QLwl"
      },
      "source": [
        "## Final datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vpFT4pKiYls"
      },
      "source": [
        "train_ds = prepare_dataset(train_df)\n",
        "validation_ds = prepare_dataset(val_df, False)\n",
        "test_ds = prepare_dataset(test_df, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SqbwhJkiav1"
      },
      "source": [
        "# Separate the train and test labels for later evaluation.\n",
        "def separate_labels(ds):\n",
        "    labels = []\n",
        "    for _, label in ds.unbatch():\n",
        "        labels.append(label)\n",
        "    labels = np.array(labels)\n",
        "    return labels\n",
        "\n",
        "train_labels = separate_labels(train_ds)\n",
        "test_labels = separate_labels(test_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_Ng98GvQOKX"
      },
      "source": [
        "## Model utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hzsPLgNQPu0"
      },
      "source": [
        "`project_embeddings()`, `create_vision_encoder()`, and `create_text_encoder()` come from [here](https://keras.io/examples/nlp/nl_image_search/). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbYj7lBO8mdo"
      },
      "source": [
        "def project_embeddings(\n",
        "    embeddings, num_projection_layers, projection_dims, dropout_rate\n",
        "):\n",
        "    projected_embeddings = keras.layers.Dense(units=projection_dims)(embeddings)\n",
        "    for _ in range(num_projection_layers):\n",
        "        x = tf.nn.gelu(projected_embeddings)\n",
        "        x = keras.layers.Dense(projection_dims)(x)\n",
        "        x = keras.layers.Dropout(dropout_rate)(x)\n",
        "        x = keras.layers.Add()([projected_embeddings, x])\n",
        "        projected_embeddings = keras.layers.LayerNormalization()(x)\n",
        "    return projected_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3OHyy8x86Is"
      },
      "source": [
        "def create_vision_encoder(\n",
        "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
        "):\n",
        "    # Load the pre-trained ResNet50V2 model to be used as the base encoder.\n",
        "    resnet_v2 = keras.applications.ResNet50V2(\n",
        "        include_top=False, weights=\"imagenet\", pooling=\"avg\"\n",
        "    )\n",
        "    # Set the trainability of the base encoder.\n",
        "    for layer in resnet_v2.layers:\n",
        "        layer.trainable = trainable\n",
        "    \n",
        "    # Receive the images as inputs.\n",
        "    image_1 = keras.Input(shape=(128, 128, 3), name=\"image_1\")\n",
        "    image_2 = keras.Input(shape=(128, 128, 3), name=\"image_2\")\n",
        "    \n",
        "    # Preprocess the input image.\n",
        "    preprocessed_1 = keras.applications.resnet_v2.preprocess_input(image_1)\n",
        "    preprocessed_2 = keras.applications.resnet_v2.preprocess_input(image_2)\n",
        "    \n",
        "    # Generate the embeddings for the images using the resnet_v2 model\n",
        "    # concatenate them.\n",
        "    embeddings_1 = resnet_v2(preprocessed_1)\n",
        "    embeddings_2 = resnet_v2(preprocessed_2)\n",
        "    embeddings = keras.layers.Concatenate()([embeddings_1, embeddings_2])\n",
        "    \n",
        "    # Project the embeddings produced by the model.\n",
        "    outputs = project_embeddings(\n",
        "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
        "    )\n",
        "    # Create the vision encoder model.\n",
        "    return keras.Model([image_1, image_2], outputs, name=\"vision_encoder\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qKs0s-b-0F5"
      },
      "source": [
        "def create_text_encoder(\n",
        "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
        "):\n",
        "    # Load the pre-trained BERT model to be used as the base encoder.\n",
        "    bert = hub.KerasLayer(\n",
        "        bert_model_path,\n",
        "        name=\"bert\",\n",
        "    )\n",
        "    # Set the trainability of the base encoder.\n",
        "    bert.trainable = trainable\n",
        "    \n",
        "    # Receive the text as inputs.\n",
        "    bert_input_features = ['input_type_ids', 'input_mask', 'input_word_ids']\n",
        "    inputs = {\n",
        "        feature: keras.Input(shape=(128, ), dtype=tf.int32, name=feature)\n",
        "        for feature in bert_input_features\n",
        "    }\n",
        "    \n",
        "    # Generate embeddings for the preprocessed text using the BERT model.\n",
        "    embeddings = bert(inputs)[\"pooled_output\"]\n",
        "    \n",
        "    # Project the embeddings produced by the model.\n",
        "    outputs = project_embeddings(\n",
        "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
        "    )\n",
        "    # Create the text encoder model.\n",
        "    return keras.Model(inputs, outputs, name=\"text_encoder\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxNkEVGgicwc"
      },
      "source": [
        "def create_multimodal_model(num_projection_layers=1, projection_dims=256, dropout_rate=0.1, \n",
        "                     vision_trainable=False, text_trainable=False, attention=False):\n",
        "    # Receive the images as inputs.\n",
        "    image_1 = keras.Input(shape=(128, 128, 3), name=\"image_1\")\n",
        "    image_2 = keras.Input(shape=(128, 128, 3), name=\"image_2\")\n",
        "\n",
        "    # Receive the text as inputs.\n",
        "    bert_input_features = ['input_type_ids', 'input_mask', 'input_word_ids']\n",
        "    text_inputs = {\n",
        "        feature: keras.Input(shape=(128, ), dtype=tf.int32, name=feature)\n",
        "        for feature in bert_input_features\n",
        "    }\n",
        "\n",
        "    # Create the encoders.\n",
        "    vision_encoder = create_vision_encoder(num_projection_layers, projection_dims, dropout_rate, vision_trainable)\n",
        "    text_encoder = create_text_encoder(num_projection_layers, projection_dims, dropout_rate, text_trainable)\n",
        "\n",
        "    # Fetch the embedding projections.\n",
        "    vision_projections = vision_encoder([image_1, image_2])\n",
        "    text_projections = text_encoder(text_inputs)\n",
        "\n",
        "    # Cross-attention.\n",
        "    if attention:\n",
        "        query_value_attention_seq = keras.layers.Attention(use_scale=True, dropout=0.2)(\n",
        "            [vision_projections, text_projections]\n",
        "        )\n",
        "\n",
        "    # Concatenate the projections and pass through the classification layer.\n",
        "    concatenated = keras.layers.Concatenate()([vision_projections, text_projections])\n",
        "    if attention:\n",
        "        concatenated = keras.layers.Concatenate()([concatenated, query_value_attention_seq])\n",
        "    outputs = keras.layers.Dense(3, activation=\"softmax\")(concatenated)\n",
        "    return keras.Model([image_1, image_2, text_inputs], outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DH1I_yxQcUO"
      },
      "source": [
        "## Model with cross-attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eefKljH9iuyt",
        "outputId": "421543bf-961e-4519-a211-98ab2fe91421"
      },
      "source": [
        "multimodal_model = create_multimodal_model(attention=True)\n",
        "multimodal_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\")\n",
        "history = multimodal_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=validation_ds,\n",
        "    epochs=10\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "38/38 [==============================] - 25s 454ms/step - loss: 0.8467 - accuracy: 0.8271 - val_loss: 0.6523 - val_accuracy: 0.8095\n",
            "Epoch 2/10\n",
            "38/38 [==============================] - 16s 405ms/step - loss: 0.4565 - accuracy: 0.8755 - val_loss: 0.5520 - val_accuracy: 0.8571\n",
            "Epoch 3/10\n",
            "38/38 [==============================] - 15s 394ms/step - loss: 0.3701 - accuracy: 0.8814 - val_loss: 0.5522 - val_accuracy: 0.8571\n",
            "Epoch 4/10\n",
            "38/38 [==============================] - 15s 398ms/step - loss: 0.3023 - accuracy: 0.8981 - val_loss: 0.7031 - val_accuracy: 0.8571\n",
            "Epoch 5/10\n",
            "38/38 [==============================] - 16s 424ms/step - loss: 0.1828 - accuracy: 0.9323 - val_loss: 0.8307 - val_accuracy: 0.8254\n",
            "Epoch 6/10\n",
            "38/38 [==============================] - 16s 407ms/step - loss: 0.1377 - accuracy: 0.9524 - val_loss: 0.8676 - val_accuracy: 0.8730\n",
            "Epoch 7/10\n",
            "38/38 [==============================] - 15s 398ms/step - loss: 0.0761 - accuracy: 0.9774 - val_loss: 1.0029 - val_accuracy: 0.8571\n",
            "Epoch 8/10\n",
            "38/38 [==============================] - 15s 393ms/step - loss: 0.0554 - accuracy: 0.9783 - val_loss: 1.0146 - val_accuracy: 0.8413\n",
            "Epoch 9/10\n",
            "38/38 [==============================] - 16s 424ms/step - loss: 0.0451 - accuracy: 0.9816 - val_loss: 1.0722 - val_accuracy: 0.8095\n",
            "Epoch 10/10\n",
            "38/38 [==============================] - 16s 409ms/step - loss: 0.0614 - accuracy: 0.9783 - val_loss: 1.1883 - val_accuracy: 0.7460\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0nMGoEQi0y8",
        "outputId": "c0515e33-3776-463c-eeb3-ba2ace02e8f6"
      },
      "source": [
        "_, acc = multimodal_model.evaluate(test_ds)\n",
        "print(f\"Accuracy on the test set: {round(acc * 100, 2)}%.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 3s 394ms/step - loss: 1.0132 - accuracy: 0.7929\n",
            "Accuracy on the test set: 79.29%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oO4sQ9VQegQ"
      },
      "source": [
        "## Model without cross-attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--dYfsnFjoNZ",
        "outputId": "5a39ff19-2757-4ed3-d379-a748a97603dd"
      },
      "source": [
        "multimodal_model = create_multimodal_model(attention=False)\n",
        "multimodal_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\")\n",
        "history = multimodal_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=validation_ds,\n",
        "    epochs=10\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "38/38 [==============================] - 23s 434ms/step - loss: 1.1289 - accuracy: 0.8087 - val_loss: 0.6368 - val_accuracy: 0.8571\n",
            "Epoch 2/10\n",
            "38/38 [==============================] - 16s 420ms/step - loss: 0.4110 - accuracy: 0.8822 - val_loss: 0.6254 - val_accuracy: 0.8095\n",
            "Epoch 3/10\n",
            "38/38 [==============================] - 16s 407ms/step - loss: 0.3799 - accuracy: 0.8797 - val_loss: 0.6159 - val_accuracy: 0.8571\n",
            "Epoch 4/10\n",
            "38/38 [==============================] - 15s 393ms/step - loss: 0.3216 - accuracy: 0.9023 - val_loss: 0.5869 - val_accuracy: 0.8571\n",
            "Epoch 5/10\n",
            "38/38 [==============================] - 15s 398ms/step - loss: 0.2180 - accuracy: 0.9223 - val_loss: 0.9261 - val_accuracy: 0.8571\n",
            "Epoch 6/10\n",
            "38/38 [==============================] - 16s 424ms/step - loss: 0.1514 - accuracy: 0.9465 - val_loss: 0.9612 - val_accuracy: 0.8571\n",
            "Epoch 7/10\n",
            "38/38 [==============================] - 16s 411ms/step - loss: 0.1229 - accuracy: 0.9574 - val_loss: 0.9977 - val_accuracy: 0.8095\n",
            "Epoch 8/10\n",
            "38/38 [==============================] - 16s 421ms/step - loss: 0.0965 - accuracy: 0.9699 - val_loss: 1.1431 - val_accuracy: 0.8254\n",
            "Epoch 9/10\n",
            "38/38 [==============================] - 15s 400ms/step - loss: 0.0742 - accuracy: 0.9724 - val_loss: 1.3899 - val_accuracy: 0.8413\n",
            "Epoch 10/10\n",
            "38/38 [==============================] - 15s 392ms/step - loss: 0.0717 - accuracy: 0.9783 - val_loss: 1.4458 - val_accuracy: 0.7778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjI9GTSujzJk",
        "outputId": "36e17e57-03ad-4471-e2e2-d9e1fd80504e"
      },
      "source": [
        "_, acc = multimodal_model.evaluate(test_ds)\n",
        "print(f\"Accuracy on the test set: {round(acc * 100, 2)}%.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 2s 316ms/step - loss: 1.3787 - accuracy: 0.7786\n",
            "Accuracy on the test set: 77.86%.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}